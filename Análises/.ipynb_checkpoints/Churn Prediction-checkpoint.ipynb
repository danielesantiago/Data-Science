{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b3190a",
   "metadata": {},
   "source": [
    "# Churn Prediction\n",
    "\n",
    "**Churn Prediction**, ou previsão de rotatividade de clientes, é uma técnica que utiliza **análises de dados para prever quais clientes têm maior probabilidade de cancelar um serviço** ou deixar de comprar um produto. Essa técnica é amplamente utilizada por empresas de diversos setores, como telecomunicações, varejo, serviços financeiros e tecnologia, para evitar a perda de clientes e melhorar a satisfação e fidelização dos mesmos.\n",
    "A previsão de churn é importante porque a **rotatividade** de clientes pode ser **prejudicial** para uma empresa, podendo resultar em **perda de receitas**, **redução da participação de mercado** e **aumento dos custos de aquisição de clientes**. Além disso, a perda de clientes pode ser um sinal de **problemas em processos de atendimento** ao cliente ou produtos, que podem ser **corrigidos** para **melhorar a qualidade geral da empresa**.\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"Arquivos/churn.jpg\" width=80%>\n",
    "</p>\n",
    "\n",
    "Ao prever quais clientes são mais propensos a cancelar um serviço ou deixar de comprar um produto, as empresas podem tomar **medidas preventivas para reduzir a rotatividade**. Isso pode incluir a criação de campanhas de retenção, melhoria da experiência do cliente e oferta de incentivos para permanecerem clientes.\n",
    "**O objetivo do projeto é prever o churn utilizando avaliações estatísticas e algoritmos de Machine Learning.** Com isso, visamos **ajudar a empresa a entender melhor seus clientes e suas necessidades**, **identificar problemas** em seus processos de atendimento ao cliente ou produtos e **implementar melhorias significativas** em toda a empresa. Isso pode levar a uma **melhoria da experiência do cliente** e **aumentar a fidelização** e **satisfação do cliente**, resultando em uma **redução da rotatividade** e um **aumento das receitas**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eea281",
   "metadata": {},
   "source": [
    "## Aquisição dos Dados\n",
    "\n",
    "Os dados utilizados neste projeto foram originalmente disponibilizados na [plataforma de ensino da IBM Developer](https://developer.ibm.com/technologies/data-science/patterns/predict-customer-churn-using-watson-studio-and-jupyter-notebooks/#), e tratam de um problema típico de uma companhia de telecomunicações. O *dataset* completo pode ser encontrado [neste link](https://raw.githubusercontent.com/carlosfab/dsnp2/master/datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv).\n",
    "\n",
    "Apesar de não haver informações explícitas disponíves, os nomes das colunas permitem um entendimento a respeito do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "4a85d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importação de bibliotecas para machine learning e pré-processamento\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, RepeatedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Importação de bibliotecas para visualização de resultados\n",
    "import scikitplot as skplt\n",
    "\n",
    "# Importação de bibliotecas para classificadores\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Importação de bibliotecas para balanceamento de classes\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Importação de bibliotecas para métricas de avaliação\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "e6de2b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importar os dados\n",
    "DATA_PATH = \"https://raw.githubusercontent.com/carlosfab/dsnp2/master/datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b29aac",
   "metadata": {},
   "source": [
    "## Análise dos Dados\n",
    "\n",
    "Esta etapa tem por objetivo permitir um entendimento de como os dados estão estruturados.\n",
    "\n",
    "**Dicionário das variáveis**\n",
    "* `customerID`: identificador único do cliente\n",
    "* `gender`: gênero do cliente (Male/Female)\n",
    "* `SeniorCitizen`: variável binária que indica se o cliente é idoso ou aposentado (1) ou não (0)\n",
    "* `Partner`: variável binária que indica se o cliente tem um parceiro (Yes) ou não (No)\n",
    "* `Dependents`: variável binária que indica se o cliente tem dependentes (Yes) ou não (No)\n",
    "* `tenure`: quantidade de meses que o cliente tem sido um assinante dos serviços da empresa\n",
    "* `PhoneService`: variável binária que indica se o cliente tem serviço de telefone (Yes) ou não (No)\n",
    "* `MultipleLines`: variável que indica se o cliente tem várias linhas telefônicas (Yes), uma única linha (No) ou nenhum serviço de telefone (No phone service)\n",
    "* `InternetService`: variável que indica se o cliente tem serviço de internet com fibra óptica (Fiber optic), com a tecnologia DSL (DSL) ou sem serviço de internet (No)\n",
    "* `OnlineSecurity`: variável que indica se o cliente tem segurança online (Yes), sem serviço de internet (No internet service) ou sem serviço de segurança (No)\n",
    "* `OnlineBackup`: variável que indica se o cliente tem backup online (Yes), sem backup (No) ou sem serviço de internet (No internet service)\n",
    "* `DeviceProtection`: variável que indica se o cliente tem proteção de dispositivo (Yes), sem proteção (No) ou sem serviço de internet (No internet service)\n",
    "* `TechSupport`: variável que indica se o cliente tem suporte técnico (Yes), sem suporte (No) ou sem serviço de internet (No internet service)\n",
    "* `StreamingTV`: variável que indica se o cliente tem serviço de streaming de TV (Yes), sem serviço de streaming de TV (No) ou sem serviço de internet (No internet service)\n",
    "* `StreamingMovies`: variável que indica se o cliente tem serviço de streaming de filmes (Yes), sem serviço de streaming de filmes (No) ou sem serviço de internet (No internet service)\n",
    "* `Contract`: variável que indica o tipo de contrato que o cliente tem com a empresa (Mensal, anual ou bianual)\n",
    "* `PaperlessBilling`: variável binária que indica se o cliente optou por fatura eletrônica (Yes) ou não (No)\n",
    "* `PaymentMethod`: variável que indica o método de pagamento que o cliente usa (Transferência bancária automática, Cartão de crédito automático, Cheque eletrônico ou Cheque enviado)\n",
    "* `MonthlyCharges`: valor mensal cobrado ao cliente\n",
    "* `TotalCharges`: valor total cobrado do cliente durante o tempo que foi assinante\n",
    "* `Churn`: variável binária que indica se o cliente cancelou o serviço (Yes) ou não (No)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c4ca7",
   "metadata": {},
   "source": [
    "## Análise Exploratória dos Dados\n",
    "\n",
    "Na análise exploratória de dados será efetuada uma explanação inicial das variáveis contidas na dataset, e entenderemos dados referentes a:\n",
    "\n",
    "* *Contexto geral das variáveis*\n",
    "* *Porcentagem de itens nulos*\n",
    "* *Balanceamento do dataset*\n",
    "* *Cancelamento em relação ao gênero*\n",
    "* *Cancelamento em relação ao valor pago mensalmente*\n",
    "* *Boxplot para variáveis numéricas*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37320311",
   "metadata": {},
   "source": [
    "Primeiramente, observaremos como estão dispostas as cinco primeiras entradas do dataset para ter uma visão ampla da estrutura de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "a3632d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver as 5 primeiras entradas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "625eb249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar o tamanho do dataframe\n",
    "print(\"Entradas: \", df.shape[0])\n",
    "print(\"Variáveis: \", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "3b27d937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extrair o nome das colunas\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "1c173258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extrair o tipo das variáveis\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821b5d3",
   "metadata": {},
   "source": [
    "Pode-se perceber que a variável `TotalCharges`, que representa o **valor total** que foi cobrado ao cliente, está como object, mas deveria ser **float**. Não é possível fazer a conversão imediata pois há uma string ' ' que representa a ausência de valores. Portanto, a mudança será feita por uma função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "f5014296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter_str_float(entrada):\n",
    "    \"\"\"\n",
    "    Converte um objeto do tipo string em float.\n",
    "\n",
    "    # Arguments\n",
    "        entrada: string, string da coluna TotalCharges.\n",
    "\n",
    "    # Returns\n",
    "        Valor convertido em float, caso permita a conversão.\n",
    "        NaN, caso o valor não seja apropriado.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return float(entrada)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "6b9d854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TotalCharges = df.TotalCharges.map(converter_str_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "aa9e9a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.TotalCharges.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a9fde",
   "metadata": {},
   "source": [
    "A coluna foi convertida, onde 11 valores estavam nulos, irei preencher os mesmos com a mediana e por fim, extrair o tipo da variável e a porcentagem de itens nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "9a3a3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.TotalCharges.fillna(df.TotalCharges.median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "12cb0c40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extrair o tipo das variáveis\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11302928",
   "metadata": {},
   "source": [
    "Podemos perceber que a coluna `TotalCharges` agora é de fato um float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "c0207422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Porcentagem de itens nulos\n",
    "round((df.isnull().sum()/df.shape[0]).sort_values(ascending = True) * 100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71903f5",
   "metadata": {},
   "source": [
    "A porcentagem de itens nulos é 0% para todas as colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "961eb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de dados distribuidos por classes\n",
    "df.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "a3d3ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porcentagem de dados distribuidos por classes\n",
    "round(df.Churn.value_counts()/df.shape[0] * 100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "455b95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar um gráfico de barras para ver o balanceamento do *dataset*\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "sns.set_palette(sns.color_palette(\"Spectral\"))\n",
    "count = df['Churn'].value_counts()\n",
    "sns.countplot(x='Churn', data=df)\n",
    "ax.set_xlabel('Churn')\n",
    "ax.set_ylabel('Quantidade')\n",
    "ax.set_title('Quantidade de Cancelamento');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea3a9c",
   "metadata": {},
   "source": [
    "O dataset é  **desbalanceado**, com 26,53% de churn. Uma consequência de ter dados desbalanceados é um **modelo enviesado**, que privilegia a classe maioritária. Logo, é necessário um **balanceamento** a fim de construir um modelo eficaz ao problema. Verificaremos agora se há um **balanceamento entre os gêneros** e se há relação entre **gênero e cancelamento**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "ebf5d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de dados distribuidos por classes\n",
    "df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "707baca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Porcentagem de dados distribuidos por classes\n",
    "round(df.gender.value_counts()/df.shape[0] * 100,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "be927ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxa de cancelamento por gênero\n",
    "df['gender'].loc[df.Churn == 'Yes'].value_counts()/df.gender.value_counts() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "35b06279",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotar um gráfico de barras para ver a quantidade de canelamento por gênero\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "sns.set_palette(sns.color_palette(\"Spectral\"))\n",
    "count = df['gender'].loc[df.Churn == 'Yes'].value_counts()\n",
    "sns.countplot(x='gender', data=df)\n",
    "ax.set_xlabel('gender')\n",
    "ax.set_ylabel('Quantidade')\n",
    "ax.set_title('Quantidade de Cancelamento por Gênero');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309b553",
   "metadata": {},
   "source": [
    "Há medidas bastante semelhante, a quantidade de mulheres e homens está **balanceada**, e não há mudança significativa na taxa de cancelamento observando o gênero do cliente. Verificaremos agora se há uma relação entre o tipo de **contrato** e a quantidade de cancelamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "4202318e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Contract.loc[df.Churn == 'Yes'].value_counts()/df.Contract.value_counts() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "3eb1322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar gráficos para as informações obtidas\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,5))\n",
    "\n",
    "\n",
    "# define informações iniciais referentes à coloração\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.set_palette(sns.color_palette(\"Spectral\"))\n",
    "\n",
    "\n",
    "# gráfico da média de preço por distrito\n",
    "ax = sns.barplot(x=list(df.Contract.value_counts().values), y=list(df.Contract.value_counts().index), order= df.Contract.value_counts().sort_values(ascending = False).index, ax = ax)\n",
    "ax.set_xlabel('Quantidade de Cancelamento')\n",
    "ax.set_ylabel('Contrato')\n",
    "ax.set_title(\"Gráfico (Cancelamento x Tipo de Contrato)\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e0a31",
   "metadata": {},
   "source": [
    "Podemos perceber que de modo geral, os **contratos mensais** tendem a ter maior quantidade de **evasão** que os anuais e bianuais, que seguem a uma quantidade semelhante. Para confirmar a hipótese realizarei um teste de hipóteses a fim de verificar se a média das categorias são iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "87e1f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Crie uma tabela de contingência\n",
    "table = pd.crosstab(df['Churn'], df['Contract'])\n",
    "\n",
    "# Execute o teste de Qui-Quadrado\n",
    "chi2, _, _, expected = chi2_contingency(table)\n",
    "\n",
    "print(f\"Valor de Qui-Quadrado: {chi2}\")\n",
    "print(f\"p-valor: {p}\")\n",
    "\n",
    "# Calcule os residuais padronizados\n",
    "residuals = (table - expected) / expected**0.5\n",
    "\n",
    "print(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758364c",
   "metadata": {},
   "source": [
    "Em resumo:\n",
    "\n",
    "* Clientes com contratos Month-to-month são mais propensos a churn (Yes Churn) do que o esperado.\n",
    "* Clientes com contratos de One year são menos propensos a churn (Yes Churn) do que o esperado.\n",
    "* Clientes com contratos de Two year também são menos propensos a churn (Yes Churn) do que o esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "9f3f3dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12,6))\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "sns.set_palette(sns.color_palette(\"Spectral\"))\n",
    "sns.histplot(data=df.MonthlyCharges.loc[df.Churn == 'No'], kde=False, ax = ax[0], color=sns.color_palette()[0])\n",
    "sns.histplot(data=df.MonthlyCharges.loc[df.Churn == 'Yes'], kde=False, ax = ax[1], color=sns.color_palette()[3])\n",
    "ax[0].set_xlabel('Valor cobrado mensalmente')\n",
    "ax[0].set_ylabel('Frequência')\n",
    "ax[0].set_title('Normal');\n",
    "\n",
    "ax[1].set_xlabel('Valor cobrado mensalmente')\n",
    "ax[1].set_ylabel('Frequência')\n",
    "ax[1].set_title('Cancelamento');\n",
    "\n",
    "fig.suptitle(\"Análise de Cobrança Mensal\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3072b02",
   "metadata": {},
   "source": [
    "Iremos realizar um teste de hipóteses para entender se há relação entre o valor cobrado e a quantidade de cancelamentos.\n",
    "Primeiro, é necessário realizar um teste de hipóteses para variância.\n",
    "\n",
    "* H0: As variâncias são iguais\n",
    "* HA: As variâncias diferem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "8e73a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variância Normal:\",df.MonthlyCharges.loc[df.Churn == 'No'].var())\n",
    "print(\"Variância Churn:\", df.MonthlyCharges.loc[df.Churn == 'Yes'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "7fa66c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.MonthlyCharges.loc[df.Churn == 'No']\n",
    "Y = df.MonthlyCharges.loc[df.Churn == 'Yes']\n",
    "df1 = len(X) - 1 #grau de liberdade da primeira amostra\n",
    "df2 = len(Y) - 1 #grau de liberdade da segunda amostra\n",
    "F = X.var() / Y.var() #A maior variância tem que ir no numerador!\n",
    "alpha = 0.05\n",
    "p_value = 1 - scipy.stats.f.cdf(F, df1, df2)\n",
    "print(\"P-valor:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a7fb8",
   "metadata": {},
   "source": [
    "Como p-valor < 0.05, rejeita-se a hipótese nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "9a1261fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Média Normal:\",df.MonthlyCharges.loc[df.Churn == 'No'].mean())\n",
    "print(\"Média Churn:\", df.MonthlyCharges.loc[df.Churn == 'Yes'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbdf33c",
   "metadata": {},
   "source": [
    "Iremos realizar um teste de hipóteses para verificar se a média de preço quando há um churn é igual a média de preço quando não há cancelamentos.\n",
    "\n",
    "H0: média1 = média2\n",
    "HA: média1 != média2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "6f263550",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.MonthlyCharges.loc[df.Churn == 'No']\n",
    "Y = df.MonthlyCharges.loc[df.Churn == 'Yes']\n",
    "stats.ttest_ind(a=X, b=Y, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a38c0",
   "metadata": {},
   "source": [
    "Como p-valor < 0.05, a 95% de nível de confiança pode-se concluir que quando **há cancelamento**, os **valores** cobrados **mensalmente** são **maiores**. Isso pode acontecer por diversos motivos, como por exemplo:\n",
    "\n",
    "* Insatisfação com o produto ou serviço: quando o cliente não está satisfeito com o produto ou serviço oferecido pela empresa, ele pode cancelar e procurar um concorrente.\n",
    "\n",
    "* Mudança na situação financeira: quando o cliente enfrenta problemas financeiros, pode precisar cortar gastos e optar por cancelar um serviço que considera menos essencial.\n",
    "\n",
    "* Ofertas melhores da concorrência: quando o cliente encontra um produto ou serviço semelhante a um preço mais baixo ou com melhores condições em outra empresa, pode optar por cancelar o serviço atual e migrar para a concorrência.\n",
    "\n",
    "Logo, o **preço da mensalidade é uma variável relevante**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806517e2",
   "metadata": {},
   "source": [
    "A variável `SeniorCitizen` indica se o cliente é idoso ou aposentado. A variável `tenure` diz respeito a quantidade de meses que o cliente tem sido um assinante dos serviços da empresa. A variável `MonthlyCharges` diz respeito ao quanto o cliente paga mensalmente pelo produto e `TotalCharges` ao valor total que foi pago enquanto assinante. A seguir um gráfico em boxplot que trará algumas informações estatísticas a respeito das variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "a954c6f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10,5))\n",
    "ax1 = axs[0, 0]\n",
    "ax2 = axs[0, 1]\n",
    "ax3 = axs[1, 0]\n",
    "ax4 = axs[1, 1]\n",
    "df.tenure.plot(kind = \"box\", ax = ax1)\n",
    "df.SeniorCitizen.plot(kind = \"box\", ax = ax2)\n",
    "df.MonthlyCharges.plot(kind = \"box\", ax = ax3)\n",
    "df.TotalCharges.plot(kind = \"box\", ax = ax4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8eb9ff",
   "metadata": {},
   "source": [
    "Estatisticamente não há outliers na variáveis ou alguma informação discrepante. O tempo médio de utilizado do produto pelo usuário foi de aproximadamente 30 meses, já a mensalidade foi 70 e o valor total pago 1397. Os dados são coerentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9c1d7",
   "metadata": {},
   "source": [
    "Verificaremos agora se há algum dado altamente correlacionado com a correlação de Pearson. Isso é importante porque características altamente correlacionadas podem causar multicolinearidade em alguns modelos de aprendizado de máquina, o que, por sua vez, pode tornar o modelo instável e os coeficientes dessas características difíceis de interpretar. Além disso, a redundância nas características pode levar a um excesso de ajuste (overfitting), onde o modelo aprende demais com os dados de treinamento e tem um desempenho inferior em dados novos e não vistos. Eliminando ou combinando características correlacionadas, podemos construir um modelo mais robusto e eficiente que generaliza melhor para novos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "892ec01e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Visualizando a matriz de correlação usando um heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Matriz de Correlação\")\n",
    "plt.show()\n",
    "\n",
    "# Identifique e remova características altamente correlacionadas\n",
    "threshold = 0.85\n",
    "to_drop = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            to_drop.append(colname)\n",
    "\n",
    "df_dropped = df.drop(columns=to_drop)\n",
    "\n",
    "print(f\"Features removidas: {to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70102a1f",
   "metadata": {},
   "source": [
    "## Conjunto de Treino e Teste\n",
    "\n",
    "É importante separar o conjunto de treino e teste antes do pré-processamento porque isso evita o vazamento de dados (\"data leakage\"). O vazamento de dados ocorre quando informações do conjunto de teste, que deve idealmente simular dados novos e não vistos, influenciam de alguma forma o processo de treinamento do modelo. Se o pré-processamento for feito antes da divisão, qualquer transformação, normalização ou codificação será baseada na distribuição e nas características de todo o conjunto de dados, incluindo o conjunto de teste. Isso pode dar uma visão otimista e muitas vezes enganosa do desempenho do modelo, pois o modelo estará sendo avaliado com base em informações que, indiretamente, já \"viu\" durante o treinamento. Ao separar os conjuntos de treino e teste primeiro, garantimos que o modelo seja treinado apenas com as informações do conjunto de treino e seja validado de maneira justa com o conjunto de teste, que representa dados novos para o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7033f3f",
   "metadata": {},
   "source": [
    "#### Separando o conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "d60580a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()\n",
    "\n",
    "# criando o conjunto de teste\n",
    "test = df_clean.sample(frac=0.15, random_state=0)\n",
    "\n",
    "# verificando o conjunto\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "00dbc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop das linhas de teste\n",
    "df_clean = df_clean.drop(test.index)\n",
    "\n",
    "# verificando o shape do df\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "fde7878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetando o index dos conjuntos\n",
    "df_clean.reset_index()\n",
    "test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5769f9aa",
   "metadata": {},
   "source": [
    "## Preparar os Dados\n",
    "\n",
    "Nesta etapa de preparação dos dados, será realizado um **pré-processamento básico**, apenas com a finalidade de construir um modelo base. \n",
    "\n",
    "As features que continham 2 labels únicos, serão processadas usando-se o `LabelEncoder`, também irei utilizá-la para a variável `Contract` que se demonstrou relevante. Na sequência, serão separadas as variáveis numéricas das categóricas. As categóricas serão transformadas em variáveis dummy, para permitir a sua utilização em todos os modelos. O Label Encoding atribui um número inteiro a cada categoria exclusiva de uma variável categórica, com valores maiores para categorias que são consideradas mais importantes. O Dummy Encoding, por outro lado, transforma cada categoria exclusiva de uma variável categórica em uma nova variável binária (0 ou 1), indicando a presença ou ausência dessa categoria na observação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3974",
   "metadata": {},
   "source": [
    "Iremos remover a variável `customerID`, pois se trata apenas de um número de identificação, não sendo relevante ao modelo. Também irei remover a variável `gender` pois não demonstrou relevância em relação à variável alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "df036291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remover customerID e gender\n",
    "df_clean = df.drop('customerID', axis = 1)\n",
    "df_clean = df_clean.drop('gender', axis = 1)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "430812c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cria um dicionário para guardar cada LabelEncoder por coluna\n",
    "encoders = {}\n",
    "\n",
    "cols_to_encode = ['SeniorCitizen','Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Contract', 'Churn']\n",
    "for col in cols_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df_clean[col] = le.fit_transform(df_clean[col])\n",
    "    encoders[col] = le  # salva o encoder ajustado para uso posterior\n",
    "\n",
    "# Variáveis dummy\n",
    "cols_to_encode = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                  'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "                  'PaymentMethod']\n",
    "df_dummies = pd.get_dummies(df_clean[cols_to_encode], prefix=cols_to_encode)\n",
    "df_clean = pd.concat([df_clean, df_dummies], axis=1)\n",
    "df_clean.drop(cols_to_encode, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "df836d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "3402760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280c07e",
   "metadata": {},
   "source": [
    "### Padronizar `tenure`, `MonthlyCharges` e `TotalCharges`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b24a4b",
   "metadata": {},
   "source": [
    "O objetivo do pré-processamento de dados é garantir que as features estejam na mesma escala, garantindo um desempenho melhor do algoritmo de *machine learning*. Para padronizar as variáveis `tenure`, `MonthlyCharges` e `TotalCharges` será utilizado o *StandardScaler*. Ele ajusta os dados para que tenham uma média zero e um desvio padrão de um."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "89bf630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar as colunas\n",
    "std_scaler = StandardScaler()\n",
    "df_new = df_clean.copy()\n",
    "df_new['std_tenure'] = std_scaler.fit_transform(df_clean['tenure'].values.reshape(-1, 1))\n",
    "df_new['std_MonthlyCharges'] = std_scaler.fit_transform(df_clean['MonthlyCharges'].values.reshape(-1, 1))\n",
    "df_new['std_TotalCharges'] = std_scaler.fit_transform(df_clean['TotalCharges'].values.reshape(-1, 1))\n",
    "df_new.drop(['tenure', 'MonthlyCharges', 'TotalCharges'], axis=1, inplace=True)\n",
    "\n",
    "# ver as primeiras entradas\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18e9db",
   "metadata": {},
   "source": [
    "Com o conjunto de teste devidamente separado, e os dados posteriores padronizados, iremos criar o conjunto de treino e validação. Utilizaremos o teste para validar o modelo ao final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "82e9b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separar variáveis entre X e y\n",
    "X = df_new.drop('Churn', axis=1)\n",
    "y = df_new['Churn'] # variável alvo\n",
    "\n",
    "# dividir o dataset entre treino e validação\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, shuffle=True, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3380c1b",
   "metadata": {},
   "source": [
    "## Modelo Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "id": "4c76665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Criando o dataset no formato LightGBM\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Definindo os parâmetros\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Treinar o modelo\n",
    "clf = lgb.train(params, dtrain, 100)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred_prob = clf.predict(X_test)\n",
    "y_pred = np.round(y_pred_prob)\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize=True, cmap='YlOrRd')\n",
    "plt.title(\"LightGBM - Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def display_metrics(model_name, y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas de avaliação do modelo {model_name}:\")\n",
    "    print(\"Acurácia: {:.2f}\".format(accuracy))\n",
    "    print(\"Precisão: {:.2f}\".format(precision))\n",
    "    print(\"Recall: {:.2f}\".format(recall))\n",
    "    print(\"ROC AUC: {:.2f}\".format(roc_auc))\n",
    "    print(\"Average Precision: {:.2f}\\n\".format(avg_precision))\n",
    "    \n",
    "\n",
    "\n",
    "display_metrics(\"Baseline Model\", y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a64b5c",
   "metadata": {},
   "source": [
    "## Balanceamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c02530",
   "metadata": {},
   "source": [
    "### Amostragem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490ece4",
   "metadata": {},
   "source": [
    "Um dataset que possua mais de 50% das entradas em uma classe já é considerado desbalanceado. A maioria dos algoritmos de aprendizado de máquina funcionam bem com conjuntos de dados equilibrados, uma vez que eles buscam otimizar a precisão geral da classificação ou medidas relacionadas. Para dados **desbalanceados**, os limites de decisões estabelecidos por padrão nos algoritmos tendem a ser **enviesados em torno da classe majoritária**. Portanto a **classe minoritária tende a ser classificada incorretamente.**\n",
    "\n",
    "Uma maneira de corrigir o problema é por meio de **amostragem** que contém duas técnicas. A técnica de **over-sampling**, consiste em **gerar novos exemplos para a classe minoritária**, de forma a aumentar sua representatividade no conjunto de dados. Já o **under-sampling** **remove** instâncias da **classe majoritária**.\n",
    "\n",
    "Esses métodos de amostragem, no entanto, possuem **diversas desvantagens** (Weiss, 2004). O under-sampling descarta a exemplos da classe majoritária potencialmente úteis e pode, portanto, degradar o desempenho do classificador. Como o over-sampling introduz casos de treinamento adicionais, pode aumentar o tempo necessário para construir um classificador, e levar a casos de overfitting.\n",
    "\n",
    "Além disso, **over-sampling simples não introduzem novos dados**, apenas replicam, portanto não tratam efetivamente a \"falta de dados\". A literatura mostra que o simples over-sampling é ineficaz para melhorar o reconhecimento da classe minoritária (Ling & Li, 1998; Drummond & Holte, 2003) e por que o under-sampling parece ter vantagem sobre o over-sampling  (Chen et al., 2004). Por essas razões, neste estudo **utilizaremos o under-sampling**.\n",
    "\n",
    "\n",
    "Também será utilizado diversos modelos de classificação em *machine learning*, como **Logistic Regression**, **KNeighbors**, **Decision Tree**, **Random Forest**, **Support Vector Machine** e **XGBoost**. Usaremos a técnica de **Random Under Sampling** que o faz aleatóriamente e os testaremos nos modelos descritos anteriormente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e730d6",
   "metadata": {},
   "source": [
    "### RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c9ae5",
   "metadata": {},
   "source": [
    "O **RandomUnderSampler** é uma técnica de undersampling, usada para lidar com conjuntos de dados desbalanceados, onde uma das classes é muito menor do que as outras. A ideia por trás do undersampling é reduzir a quantidade de amostras da classe majoritária para equilibrar as proporções entre as classes. O RandomUnderSampler é uma técnica simples que **remove aleatoriamente amostras da classe majoritária** até que o número de amostras na classe majoritária corresponda ao número de amostras da classe minoritária. Essa técnica pode ser eficaz para reduzir o viés do modelo em direção à classe majoritária e melhorar a precisão das previsões para a classe minoritária."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "c3e4516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando o RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy=\"majority\")\n",
    "\n",
    "# aplicando o RandomUnderSampler ao conjunto de dados\n",
    "X_under, y_under = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a724421",
   "metadata": {},
   "source": [
    "Nas linhas acima foi criado o objeto para o RandomUnderSampler e os dados foram balanceados. Utilizou-se a estratégia de amostragem \"majority\" para apenas remover instâncias da classe majoritária. Observaremos agora o tamanho antigo da variável alvo e o novo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "e7548db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando o tamanho do conjunto de treinamento\n",
    "print(\"Tamanho do y_train:\", np.shape(y_train))\n",
    "\n",
    "# Visualizando o tamanho do conjunto balanceado\n",
    "print(\"Tamanho do y_resampled:\", np.shape(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cecc1ad",
   "metadata": {},
   "source": [
    "Pode-se observar que várias linhas foram removidas a fim de balancear os dados. Veremos a nova disposição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "20553957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver o balanceamento das classes\n",
    "print(pd.Series(y_under).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "21d9c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotar a nova distribuição de classes\n",
    "sns.countplot(x=y_under, hue=y_under);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e011f",
   "metadata": {},
   "source": [
    "Temos então, os dados balanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c5d70",
   "metadata": {},
   "source": [
    "## Treinamento dos Algoritmos de *Machine Learning* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ccc681",
   "metadata": {},
   "source": [
    "Para o **treinamento** utilizaremos diversos algoritmos de Machine Learning. Para validar se tivemos um bom resultado com o treinamento, utilizaremos algumas **métricas** para modelos de classificação, como **matriz de confusão, acurácia, recall, precision e AUC.** Sendo priorizado o **recall**, que  é a **proporção de exemplos positivos que o modelo classificou corretamente em relação a todos os exemplos positivos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe35bb7",
   "metadata": {},
   "source": [
    "Serão instanciados objetos dos algoritmos abaixo, em seguida, eles serão treinados em validação cruzada para verificar qual tem o maior recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bc1ce",
   "metadata": {},
   "source": [
    "\n",
    "1. **DecisionTreeClassifier**:\n",
    "   - **O que é**: Um classificador baseado em árvores de decisão. Funciona particionando o espaço de entrada de dados em regiões, e cada região é associada a uma classe específica.\n",
    "   - **Uso típico**: Muito útil quando você quer entender a estrutura hierárquica dos dados ou a importância das características.\n",
    "   - **Vantagens**: Fácil de visualizar e interpretar, não precisa de normalização, capaz de lidar com características numéricas e categóricas.\n",
    "   - **Desvantagens**: Pode ser propenso a overfitting, especialmente com dados ruidosos ou quando a árvore é muito profunda.\n",
    "\n",
    "2. **BalancedBaggingClassifier**:\n",
    "   - **O que é**: Uma extensão do algoritmo de Bagging que trata do desbalanceamento de classes durante o processo de treinamento.\n",
    "   - **Uso típico**: Útil quando lidamos com conjuntos de dados desbalanceados.\n",
    "   - **Vantagens**: Reduz o problema do desbalanceamento ao reamostrar os dados em cada subconjunto, potencialmente melhorando o desempenho em classes minoritárias.\n",
    "   - **Desvantagens**: Pode aumentar o risco de overfitting nas classes minoritárias se não for cuidadosamente regulado.\n",
    "\n",
    "3. **BalancedRandomForestClassifier**:\n",
    "   - **O que é**: Uma variação da floresta aleatória que cria árvores equilibradas através do reamostramento com ou sem substituição.\n",
    "   - **Uso típico**: Assim como o BalancedBaggingClassifier, é útil para conjuntos de dados desbalanceados.\n",
    "   - **Vantagens**: Combina os benefícios das florestas aleatórias (como redução da variância e aumento da robustez) com o reequilíbrio para tratar de conjuntos desbalanceados.\n",
    "   - **Desvantagens**: Como qualquer técnica de reamostragem, pode levar a um overfitting em algumas situações.\n",
    "\n",
    "4. **XGBClassifier (XGBoost)**:\n",
    "   - **O que é**: Uma implementação otimizada de árvores de aumento de gradiente.\n",
    "   - **Uso típico**: Muito popular em competições de ciência de dados devido à sua alta eficiência e desempenho.\n",
    "   - **Vantagens**: Rápido, eficiente, pode ser paralelizado/distribuído, tem uma rotina regularizada (L1 e L2) que previne o overfitting.\n",
    "   - **Desvantagens**: Pode ser sensível a ruídos, tem muitos hiperparâmetros para ajustar.\n",
    "\n",
    "5. **LightGBM**:\n",
    "   - **O que é**: Uma estrutura de aumento de gradiente que usa técnicas baseadas em árvores. É uma implementação que otimiza o uso da memória e a velocidade de treinamento.\n",
    "   - **Uso típico**: Desempenho similar ao XGBoost, mas muitas vezes é mais rápido e usa menos memória.\n",
    "   - **Vantagens**: Rápido, eficiente, suporta treinamento distribuído, otimizado para grandes conjuntos de dados, e pode lidar com dados categóricos sem precisar de codificação prévia.\n",
    "   - **Desvantagens**: Assim como o XGBoost, possui muitos hiperparâmetros, e pode ser sensível a configurações de hiperparâmetros ou ruídos nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "d8b91a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the classifiers\n",
    "balanced_bagging = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                                            random_state=42)\n",
    "balanced_rf = BalancedRandomForestClassifier(random_state=42)\n",
    "xgboost = XGBClassifier(random_state=42)\n",
    "lightgmb = lgb.LGBMClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bb432",
   "metadata": {},
   "source": [
    "A validação cruzada é uma técnica que permite avaliar o desempenho do modelo dividindo os dados em k partições (folds), treinando o modelo em k-1 partições e testando-o na partição restante, repetindo esse processo k vezes. Ela é usada para estimar a performance do modelo em dados não vistos e ajuda a evitar problemas de sobreajuste (overfitting) ou subajuste (underfitting). A utilizaremos durante o treinamento dos nossos modelos.\n",
    "\n",
    "Baseado no paper **Handling class imbalance in customer churn prediction**, [disponível neste link](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.477.1151&rep=rep1&type=pdf), utilizaremos a técnica de 5x2 CV, onde foram utilizadas cinco iterações de validação cruzada de dois folders. Em cada iteração, os dados foram divididos aleatoriamente pela metade. Uma metade foi fornecida aos algoritmos e a outra metade foi usada para testar a solução final; e vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "id": "c4854e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creation of classifiers list\n",
    "classifiers = [\n",
    "    ('Balanced Bagging', balanced_bagging),\n",
    "    ('Balanced Random Forest', balanced_rf),\n",
    "    ('XGBoost', xgboost),\n",
    "    ('LightGBM', lightgmb)\n",
    "]\n",
    "\n",
    "recall = []\n",
    "model = []\n",
    "\n",
    "# Cross-validation with two folders and 5 repetitions\n",
    "cv = RepeatedKFold(n_splits=2, n_repeats=5, random_state=42)\n",
    "\n",
    "X_under_np = X_under.to_numpy()\n",
    "y_under_np = y_under.to_numpy()\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    scores = []\n",
    "    \n",
    "    # Cross-validation\n",
    "    for train_index, test_index in cv.split(X_under_np, y_under_np):\n",
    "        # Data split into train and test\n",
    "        X_train, X_test = X_under_np[train_index], X_under_np[test_index]\n",
    "        y_train, y_test = y_under_np[train_index], y_under_np[test_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate recall and add to scores list\n",
    "        recall_iter = recall_score(y_test, y_pred, average='macro')\n",
    "        scores.append(recall_iter)\n",
    "\n",
    "    recall.append(np.mean(scores))\n",
    "    model.append(name)\n",
    "    \n",
    "recall_sorted, model_sorted = zip(*sorted(zip(recall, model), reverse=True))\n",
    "pd.DataFrame(data=recall_sorted, index=model_sorted, columns=['Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2150bf",
   "metadata": {},
   "source": [
    "No código acima, foi realizado nos algoritmos a validação cruzada com a técnica 5x2 explicitada previamente. Também foi calculado a média do recall para cada algoritmo. Os algoritmos com melhor recall foram Balanced Random Forest e LightGBM, respectivamente. Agora irei otimizar os hiperparâmetros a fim de obter resultados ainda mais satisfatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dbea2b",
   "metadata": {},
   "source": [
    "## Otimização dos Hiperparâmetros - Balanced Random Forest e LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9364e1",
   "metadata": {},
   "source": [
    "Uma das formas mais comuns de otimizar os hiperparâmetros do modelo é utilizando o GridSearchCV do scikit-learn. O GridSearchCV é uma ferramenta que permite testar várias combinações de hiperparâmetros de um modelo, treinando e avaliando cada modelo para encontrar a melhor combinação de hiperparâmetros para o conjunto de dados em questão. \n",
    "\n",
    "O utilizaremos para encontrar os melhores parâmetros dos modelos de Machine Learning escolhidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "id": "d0ed105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # For Balanced Random Forest\n",
    "# param_grid_brf = {\n",
    "#     'n_estimators': [10, 50, 100, 150],\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#     'max_depth': [None, 10, 20, 30, 40],\n",
    "# }\n",
    "\n",
    "# grid_search_brf = GridSearchCV(BalancedRandomForestClassifier(random_state=42),\n",
    "#                                param_grid=param_grid_brf,\n",
    "#                                cv=5,\n",
    "#                                scoring='recall_macro',\n",
    "#                                n_jobs=-1)\n",
    "# grid_search_brf.fit(X_under, y_under)\n",
    "# best_brf = grid_search_brf.best_estimator_\n",
    "\n",
    "# print(\"Best Parameters for Balanced Random Forest: \", grid_search_brf.best_params_)\n",
    "\n",
    "# # For LightGBM\n",
    "# param_grid_lgb = {\n",
    "#     'num_leaves': [31, 62, 93],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "# }\n",
    "\n",
    "# grid_search_lgb = GridSearchCV(lgb.LGBMClassifier(random_state=42),\n",
    "#                                param_grid=param_grid_lgb,\n",
    "#                                cv=5,\n",
    "#                                scoring='recall_macro',\n",
    "#                                n_jobs=-1)\n",
    "# grid_search_lgb.fit(X_under, y_under)\n",
    "# best_lgb = grid_search_lgb.best_estimator_\n",
    "\n",
    "# print(\"Best Parameters for LightGBM: \", grid_search_lgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c821956",
   "metadata": {},
   "source": [
    "Os modelos foram treinados, abaixo realizaremos previsões no conjunto de teste e calcularemos métricas de avaliação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88137e9",
   "metadata": {},
   "source": [
    "### Balanced Random Forest & Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "7893fad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Treinando o modelo Balanced Random Forest com os melhores parâmetros\n",
    "brf = RandomForestClassifier(max_depth=10, max_features='auto', n_estimators=100, class_weight='balanced')\n",
    "brf.fit(X_train, y_train)\n",
    "\n",
    "# Treinando o modelo LightGBM com os melhores parâmetros\n",
    "lgbm = lgb.LGBMClassifier(learning_rate=0.01, n_estimators=150, num_leaves=31)\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_brf = brf.predict(X_test)\n",
    "y_pred_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def calculate_metrics(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_pred)\n",
    "    return accuracy, precision, recall, roc_auc, avg_precision\n",
    "\n",
    "accuracy_brf, precision_brf, recall_brf, roc_auc_brf, avg_precision_brf = calculate_metrics(y_test, y_pred_brf)\n",
    "accuracy_lgbm, precision_lgbm, recall_lgbm, roc_auc_lgbm, avg_precision_lgbm = calculate_metrics(y_test, y_pred_lgbm)\n",
    "\n",
    "print(\"Métricas de avaliação do modelo Balanced Random Forest:\")\n",
    "print(\"Acurácia: {:.2f}\".format(accuracy_brf))\n",
    "print(\"Precisão: {:.2f}\".format(precision_brf))\n",
    "print(\"Recall: {:.2f}\".format(recall_brf))\n",
    "print(\"ROC AUC: {:.2f}\".format(roc_auc_brf))\n",
    "print(\"Average Precision: {:.2f}\".format(avg_precision_brf))\n",
    "print(\"\\n\")\n",
    "print(\"Métricas de avaliação do modelo LightGBM:\")\n",
    "print(\"Acurácia: {:.2f}\".format(accuracy_lgbm))\n",
    "print(\"Precisão: {:.2f}\".format(precision_lgbm))\n",
    "print(\"Recall: {:.2f}\".format(recall_lgbm))\n",
    "print(\"ROC AUC: {:.2f}\".format(roc_auc_lgbm))\n",
    "print(\"Average Precision: {:.2f}\".format(avg_precision_lgbm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745d6bd",
   "metadata": {},
   "source": [
    "**Resumo dos Resultados para o Conjunto de Treino de Churn Prediction:**\n",
    "\n",
    "Ambos os modelos, Balanced Random Forest e LightGBM, apresentaram performances semelhantes no conjunto de treino para a tarefa de Churn Prediction. Os dois modelos são comparáveis em termos de desempenho. A escolha entre eles pode depender de considerações adicionais, como a importância das métricas para o problema de negócio específico ou outros fatores, como tempo de treinamento e recursos disponíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63f94c",
   "metadata": {},
   "source": [
    "Será plotado a matriz de confusão dos resultados obtidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "f30df4af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred_brf, normalize=True, cmap='YlOrRd')\n",
    "plt.title(\"Balanced Random Forest - Confusion Matrix\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "1830a558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred_lgbm, normalize=True, cmap='YlOrRd')\n",
    "plt.title(\"Light GBM - Confusion Matrix\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055be15c",
   "metadata": {},
   "source": [
    "## Avaliação do Modelo Utilizando os Dados de Teste\n",
    "\n",
    "No começo do projeto, realizamos um split nos dados para que tivéssemos 3 conjuntos: Um de treino, um de validação e um de teste. Agora que já exploramos e preparamos os dados, instaciamos o modelo e fizemos previsões no conjunto de validação, é hora de descobrirmos o potencial desse simples modelo com os dados de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b006a5",
   "metadata": {},
   "source": [
    "### Padronização no conjunto de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefa388",
   "metadata": {},
   "source": [
    "Antes de efetivamente testar os dados nos nossos classificadores, precisamos padronizar os modelos, de forma semelhante à padronização dos dados de treinamento. Faremos isso utilizando o StandardScarler. Logo em seguida, os dados serão separados em X_test e y_test, onde y_test irá conter a variável alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "a5396479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover colunas\n",
    "test = test.drop(['customerID', 'gender'], axis=1)\n",
    "\n",
    "cols_to_encode = ['SeniorCitizen','Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Contract', 'Churn']\n",
    "\n",
    "# Usar encoders ajustados no treinamento\n",
    "for col in cols_to_encode:\n",
    "    le = encoders[col]\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "# Variáveis dummy\n",
    "cols_to_encode = ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', \n",
    "                  'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', \n",
    "                  'PaymentMethod']\n",
    "test_dummies = pd.get_dummies(test[cols_to_encode], prefix=cols_to_encode)\n",
    "test = pd.concat([test, test_dummies], axis=1)\n",
    "test.drop(cols_to_encode, axis=1, inplace=True)\n",
    "\n",
    "# Usar o scaler ajustado no treinamento\n",
    "test['std_tenure'] = std_scaler.transform(test['tenure'].values.reshape(-1, 1))\n",
    "test['std_MonthlyCharges'] = std_scaler.transform(test['MonthlyCharges'].values.reshape(-1, 1))\n",
    "test['std_TotalCharges'] = std_scaler.transform(test['TotalCharges'].values.reshape(-1, 1))\n",
    "test.drop(['tenure', 'MonthlyCharges', 'TotalCharges'], axis=1, inplace=True)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "116a0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separar variáveis entre X e y\n",
    "X_test = test.drop('Churn', axis=1)\n",
    "y_test = test['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a88d2",
   "metadata": {},
   "source": [
    "### Balanceamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef1f858",
   "metadata": {},
   "source": [
    "Verificaremos o balanceamento do conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "994b53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tamanho dos dados de teste\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "4b6f7202",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# balanceamento dos dados de teste\n",
    "test.Churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778aed62",
   "metadata": {},
   "source": [
    "O conjunto de teste contém 1056 instâncias, sendo 791 referente à classe 0 (não há Churn) e 265 à classe 1 (há churn). Pode-se perceber que o **conjunto é desbalanceado**, haja vista que é uma amostra dos dados originais. É esperado que o nosso classificador apresente resultados satisfatórios e consiga distinguir bem ambas as classes.\n",
    "\n",
    "Abaixo realizaremos previsões no conjunto de teste utilizando ambos os classificadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "da249651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def display_metrics(model_name, y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    avg_precision = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas de avaliação do modelo {model_name}:\")\n",
    "    print(\"Acurácia: {:.2f}\".format(accuracy))\n",
    "    print(\"Precisão: {:.2f}\".format(precision))\n",
    "    print(\"Recall: {:.2f}\".format(recall))\n",
    "    print(\"ROC AUC: {:.2f}\".format(roc_auc))\n",
    "    print(\"Average Precision: {:.2f}\\n\".format(avg_precision))\n",
    "    \n",
    "    \n",
    "y_pred_brf = brf.predict(X_test)\n",
    "y_pred_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "display_metrics(\"Balanced Random Forest\", y_test, y_pred_brf)\n",
    "display_metrics(\"Light GBM\", y_test, y_pred_lgbm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba993f4",
   "metadata": {},
   "source": [
    "A análise dos modelos Balanced Random Forest e Light GBM revela uma performance notável em suas previsões. Em termos de acurácia, ambos os modelos apresentaram resultados próximos, com o Balanced Random Forest tendo uma ligeira vantagem com 0.74 em comparação com o Light GBM que registrou 0.73. Em relação à precisão, os dois modelos exibiram valores semelhantes, com o Balanced Random Forest em 0.49 e o Light GBM ligeiramente inferior em 0.48.\n",
    "\n",
    "No entanto, ao considerarmos o recall - métrica que foi priorizada neste estudo - o Balanced Random Forest mostrou-se ligeiramente superior com um valor de 0.84, enquanto o Light GBM registrou 0.80. Esta diferença destaca a habilidade do Balanced Random Forest em identificar corretamente a classe positiva.\n",
    "\n",
    "Para as métricas ROC AUC e Average Precision, os modelos também apresentaram resultados próximos, indicando uma concordância geral em termos de desempenho. O Balanced Random Forest teve um ROC AUC de 0.77 e Average Precision de 0.45, enquanto o Light GBM apresentou 0.76 e 0.44, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "ef86fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred_brf, normalize=True, cmap='YlOrRd', ax = ax[0])\n",
    "skplt.metrics.plot_confusion_matrix(y_test, y_pred_lgbm, normalize=True, cmap='YlOrRd', ax = ax[1])\n",
    "\n",
    "ax[0].set_title(\"Balanced Random Forest\")\n",
    "ax[1].set_title(\"Light GBM\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c9cc6",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Com base nas análises realizadas, pode-se concluir que a escolha do **modelo de classificação**, do **pré-processamento** dos dados e a **técnica de balanceamento de classes** é fundamental para obter um bom desempenho no problema de Churn.\n",
    "\n",
    "Observou-se um desempenho semelhante no conjunto de testes e no conjunto de validação. A técnica de balanceamento undersampling foi utilizada, onde somente dados da classe majoritária foram removidos. Foi escolhida a métrica **recall** como prioridade, haja vista que prever o cancelamento de clientes possibilita à empresa realizar uma intervensão e mantê-lo. Mas se for previsto um cancelamento e de fato não houver, não há danos signficativos.\n",
    "\n",
    "Ao avaliar os modelos para prever o churn de clientes, dois algoritmos se destacaram: Balanced Random Forest e Light GBM.\n",
    "\n",
    "O modelo Balanced Random Forest apresentou um desempenho notável, com uma acurácia de 0.74, indicando que ele fez previsões corretas em 74% dos casos. No entanto, o aspecto mais impressionante deste modelo foi o seu recall de 0.84, o que significa que foi capaz de identificar corretamente 84% dos clientes em risco de churn. Dada a natureza deste problema, essa é uma métrica essencial, uma vez que a identificação e retenção de clientes em risco é uma prioridade. Apesar da sua precisão ser de 0.49, dada a importância do recall, esta é uma troca aceitável. As métricas ROC AUC e Average Precision, ambas acima de 0.75, confirmam a robustez deste modelo.\n",
    "\n",
    "Por outro lado, o Light GBM também apresentou resultados impressionantes. Com uma acurácia ligeiramente inferior de 0.73 e um recall de 0.80, o Light GBM foi quase tão eficaz quanto o Balanced Random Forest em identificar clientes em risco de churn. A precisão deste modelo foi de 0.48, e, novamente, dado o foco no recall, isso é aceitável. As métricas ROC AUC e Average Precision reiteram a qualidade deste modelo, estando ambas próximas de 0.75."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
